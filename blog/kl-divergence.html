<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
        integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // auto-render specific keys
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                    { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                    { left: "\\begin{align}", right: "\\end{align}", display: true },
                    { left: "\\begin{align*}", right: "\\end{align*}", display: true },
                    { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                    { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                ],
                // rendering keys
                throwOnError: false,
                fleqn: true // display math renders flush left with a 2em left margin   
            });
        });
    </script>


    <title>Useful KL Divergence Results</title>

</head>

<body>
    <a class="button" href="../index.html">
        <button>Home</button>
    </a>

    <section id="title">
        <h1>Useful KL Divergence Results</h1>
    </section>
    <hr>

    <section id="intro">
        <p>

            The KL divergence of distribution $p$ from $q$ is a measure of dissimilarity,
            taken as the expectation of the log difference and using $p(x)$ as the weighting function.
            It's used a lot in Bayesian inference (specifically, the divergence of a posterior $p$
            from some prior $q$), so we're listing some properties and closed-form equations here.<br><br>

            For discrete distributions over sample space $\mathcal{X}$, we have:


            \begin{equation}
            D_{KL}(p||q) = E_{x\sim p(x)}\left[\log\left(\frac{p(x)}{q(x)}\right)\right]= \sum_{x \in \mathcal{X}}
            p(x)\log\left(\frac{p(x)}{q(x)}\right)
            \end{equation}

            whereas for continuous distributions we have the integral:

            \begin{equation}
            D_{KL}(p||q) = \int_{-\infty}^{\infty} p(x)\log\left(\frac{p(x)}{q(x)}\right)dx
            \end{equation}

            When we use the base 2 log, the KL divergence has units of "bits"; "nats" for natural log.

        </p>

    </section>

    <hr>

    <section>
        <h3>Properties</h3>

        <ol>
            <li>
                $D_{KL}(p||q) = 0$ if and only if $p=q$. <br><br>

                If $p=q$ then $\log p(x)/q(x) = \log(1) = 0$
                for all $x$ and $D_{KL}(p||q)=0$. The proof of the forward implication
                is a little bit trickier.

            </li><br>
            <li>
                Non-negativity ($D_{KL} \geq 0$).<br><br>

                Since $\log(a) \leq a-1$ for all $a > 0$, we have:<br>

                \begin{align}
                D_{KL}(p||q)&=-\sum p(x) \log \left(\frac{q(x)}{p(x)}\right) \tag{Invert the log} \\
                &\geq -\sum p(x)\left(\frac{q(x)}{p(x)} - 1\right) \tag{letting $a=q/p$}
                \end{align}

                The right hand side is equal to zero, which completes the proof:
                \begin{align}
                -\sum p(x)\left(\frac{q(x)}{p(x)} - 1\right) &=-\sum q(x) + \sum p(x) \tag*{}\\
                &=-1 + 1 \tag*{}\\
                &=0 \tag*{}\\
                D_{KL}(p||q) &\geq 0 \tag*{}
                \end{align}
            </li><br>
            <li>
                However the KL divergence is asymmetric.

                \begin{equation}
                \tag*{}
                D_{KL}(p||q) \neq D_{KL}(q||p)
                \end{equation}

            </li><br>
            <li>
                And it doesn't satisfy the triangle inequality.

                \begin{equation}
                \tag*{}
                D_{KL}(p||r) \nleq D_{KL}(p||q) + D_{KL}(q||r) 
                \end{equation}

                for all distributions $p$, $q$ and $r$
            </li><br>
        </ol>
        <p>
            <b>Remark</b>: Due to the last two properties, the KL divergence doesn't qualify as a distance metric,
            which is why it's called the KL <i>divergence</i>. Also, because of asymmetry, we usually say that
            $D_{KL}(p||q)$ is the "divergence of $p$ from $q$" rather than "between $p$ and $q$".
        </p>

    </section>

    <hr>

    <section>
        <h3>Some Useful Results</h3>

        <p>
            In variational Bayesian inference we often encounter the KL divergence of a Gaussian
            from another Gaussian. As you'd expect, the algebra works out nicely to closed-formed
            equations. We list them here for the univariate (Eq. 3) and multivariate (Eq. 4) cases ; and
            for the special case of an isotropic Gaussian posterior from a standard normal prior (Eq. 5),
            and from a uniform prior (Eq. 6).
        </p>


        <subsection id="univariate">
            <p>
                $\underline{
                p(x)=N(\mu_1, \sigma_1^2), q(x)=N(\mu_2, \sigma_2^2)
                } \\$ 
                <br>
                The KL divergence of an univariate Gaussian posterior $p$ from a Gaussian prior $q$ is given by Eq. (3).
            </p>
            <p>
                \begin{align*}

                D_{KL}(p||q) &= \int_{-\infty}^{\infty} p(x)\log\left(\frac{p(x)}{q(x)}\right)dx \\


                &= \int_{-\infty}^{\infty} p(x) \log
                \left(
                \frac{\sigma_2}{\sigma_1} \exp
                \left[
                -\frac{(x-\mu_1)^2}{2\sigma_1^2} + \frac{(x-\mu_2)^2}{2\sigma_2^2}
                \right]
                \right) dx \\

                
                &=\ln \left( \frac{\sigma_2}{\sigma_1} \right) 
                - \frac{1}{2\sigma_1^2}
                \underbrace{\int_{-\infty}^{\infty}p(x)(x-\mu_1)^2dx}_{\sigma_1^2} \\
                \tag{Use ln}
                & \quad + \frac{1}{2\sigma_2^2} 
                
                \int_{-\infty}^{\infty}p(x)(x-\mu_2)^2dx \\

                
                



                &=\ln \left( \frac{\sigma_2}{\sigma_1} \right) - \frac{1}{2} +
                \frac{1}{2\sigma_2^2} \int_{-\infty}^{\infty}p(x)(x^2-2\mu_2x + \mu_2^2)dx \\


                &=\ln \left( \frac{\sigma_2}{\sigma_1} \right) - \frac{1}{2} +
                \frac{1}{2\sigma_2^2}
                \left(
                E_{x \sim p}[x^2] - 2\mu_1\mu_2 + \mu_2^2
                \right) \\

                \tag{$E[x^2] = \sigma^2 + \mu^2$}
                &=\ln \left( \frac{\sigma_2}{\sigma_1} \right) - \frac{1}{2} +
                \frac{1}{2\sigma_2^2}
                \left(
                \sigma_1^2 + \mu_z^2 - 2\mu_1\mu_2 + \mu_2^2
                \right) \\\\

                \tag{3}
                D_{KL}(p||q) &=\ln \left( \frac{\sigma_2}{\sigma_1} \right) - \frac{1}{2} +
                \frac{1}{2\sigma_2^2}
                \left(
                \sigma_1^2 + (\mu_1 - \mu_2)^2
                \right) \\

                \end{align*}

            </p><br>
        </subsection>

        <subsection id="univariate standard normal">
            <p>
                $\underline{
                    p(x)=N(\mu_1, \sigma_1^2), q(x)=N(0, 1)
                }$
                <br><br>
                For a standard normal prior, $D_{KL}$ only depends on the mean and variance of the posterior,
                $\mu_1$ and $\sigma_1^2$.

                \begin{align*}
                &\mu_2 = 0 \text{ and } \sigma_2=1 \\
                &D_{KL}(p||q) = -\ln(\sigma_1) - \frac{1}{2} + \frac{1}{2}(\sigma_1^2 + \mu_1^2)
                \end{align*}
            </p><br>
        </subsection>

        <subsection id="multivariate">

            <p>
                $\underline{
                p(\bold{x})=N(\bold{\mu_0}, \bold{\Sigma_0}), q(\bold{x})=N(\bold{\mu_1}, \bold{\Sigma_1})
                }$
                <br><br>
                For multivariate Gaussian distributions we have:

                \begin{align*}
                D_{KL}(p||q) &= \int_{-\infty}^{\infty} p(\bold{x})\log\left(\frac{p(\bold{x})}{q(\bold{x})}\right)d\bold{x} \\
                &=\int_{-\infty}^{\infty}p(\bold{x})
                \left(
                    \frac{1}{2}\ln
                    \left(
                        \frac{\begin{vmatrix}\bold{\Sigma_2}\end{vmatrix}}{\begin{vmatrix}\bold{\Sigma_1}\end{vmatrix}}
                    \right)
                    - \frac{1}{2}(\bold{x}-\bold{\mu_1})^T\bold{\Sigma_1^{-1}}(\bold{x}-\bold{\mu_1})
                    + \frac{1}{2}(\bold{x}-\bold{\mu_2})^T\bold{\Sigma_2^{-1}}(\bold{x}-\bold{\mu_2}) 
                \right) d\bold{x} \\

                &=\frac{1}{2}\ln
                \left(
                    \frac{\begin{vmatrix}\bold{\Sigma_2}\end{vmatrix}}{\begin{vmatrix}\bold{\Sigma_1}\end{vmatrix}}
                \right)
                -\frac{1}{2}
                \underbrace{
                E_{\bold{x}\sim p(\bold{x})}\left[(\bold{x}-\bold{\mu_1})^T\bold{\Sigma_1^{-1}}(\bold{x}-\bold{\mu_1})\right]
                }_{\text{(a)}}
                
                +\frac{1}{2}
                \underbrace{
                E_{\bold{x}\sim p(\bold{x})}\left[(\bold{x}-\bold{\mu_2})^T\bold{\Sigma_2^{-1}}(\bold{x}-\bold{\mu_2})\right]
                }_{\text{(b)}}
                \end{align*}
            </p>

            <p>
                We use the trace trick for the expectation of quadratic forms. First, the quadratic forms
                inside (a) and (b) evaluate to 1 by 1 matrices. Since a 1 by 1 matrix only has one entry,
                it's equal to the trace of itself. We can write (a) as <br>

                \begin{align*}
                
                E_{\bold{x}\sim p(\bold{x})}
                \left[ tr
                    \left(
                        (\bold{x}-\bold{\mu_1})^T\bold{\Sigma_1^{-1}}(\bold{x}-\bold{\mu_1})
                    \right)
                \right] 
                \end{align*} 

                Now, traces are invariant under cyclic permutation, i.e. $tr(ABC)=tr(BCA)=tr(CAB)$, and
                the expectation of the trace of a matrix is equal to the trace of the expectation,
                $E[tr(A)]=tr(E[A])$. Putting these together we have
                \begin{align*}
                tr\left(
                    E_{\bold{x}\sim p(\bold{x})}
                    \left[ 
                    (\bold{x}-\bold{\mu_1})(\bold{x}-\bold{\mu_1})^T\bold{\Sigma_1^{-1}}
                    \right] 
                \right)
                \end{align*}

                Since the inverse covariance matrix is constant w.r.t. $\bold{x}$, we move it out of the 
                expectation and find that (a) equals $n$.

                \begin{align*}
                tr\left(
                    \bold{\Sigma_1^{-1}}
                    E_{\bold{x}\sim p(\bold{x})}
                    \left[ 
                    (\bold{x}-\bold{\mu_1})(\bold{x}-\bold{\mu_1})^T
                    \right] 
                \right)
                = tr\left(
                    \bold{\Sigma_1^{-1}}\bold{\Sigma_1}
                \right)
                = tr\left(\bold{I}\right)
                = n
                
                \end{align*}

                Evaluating (b) uses the same trick but takes a little bit more work. Start by introducing $\bold{\mu_1}$:

                \begin{align*}

                
                

                \tag{Introduce $\bold{\mu_1}$}
                E_{\bold{x}\sim p(\bold{x})}
                \left[
                    ((\bold{x}-\bold{\mu_1})-(\bold{\mu_1} - \bold{\mu_2}))^T
                    \bold{\Sigma_2^{-1}}(\bold{x}-\bold{\mu_2})
                \right]
                &=
                
                


                \end{align*}
                
                

            <!-- TODO: make \text font be Sans Pro -->

            </p>
        </subsection>

        <subsection id="multivariate standard normal">
            <p>
                $\underline{
                p(\bold{x})=N(\bold{\mu}, diag(\sigma_1^2, ..., \sigma_n^2)), q(\bold{x})=N(\bold{0}, I)
                }$
            </p>
        </subsection>


        <subsection id="uniform">
            <p>
                $\underline{
                p(\bold{x})=N(\bold{\mu}, diag(\sigma_1^2, ..., \sigma_n^2)), q(\bold{x})=U(a, b)
                }$
            </p>
        </subsection>















    </section>






</body>

</html>