<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
        integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // auto-render specific keys
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                    { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                    { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
                    { left: "\\begin{align}", right: "\\end{align}", display: true },
                    { left: "\\begin{align*}", right: "\\end{align*}", display: true },
                    { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                    { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                ],
                // rendering keys
                throwOnError: false,
                fleqn: true // display math renders flush left with a 2em left margin   
            });
        });
    </script>


    <title>Latent Spaces and the ELBO</title>

</head>

<body>
    <a class="button" href="../index.html">
        <button>Home</button>
    </a>

    <section id="title">
        <h1>Latent Spaces, the ELBO and Generative Models</h1>
    </section>

    <hr>

    <!-- Define variables -->
    $$
    \gdef\expect{E_{\bold{z} \sim q_\phi}}
    $$

    <section id="intro">
        <p>
            Here we'll make some remarks about latent spaces, derive the ELBO, and 
            talk about the goal of generative modeling from the perspective of VAEs
            <a href="https://arxiv.org/abs/1312.6114"> (Kingma & Welling, 2013)</a>. 
            <br><br>

            The ELBO is used in variational Bayesian inference as a proxy 
            for learning a model of the true posterior over latent variables.
            The framework assumes that in the real world, an observation 
            $\bold{x}$ is generated by some underlying set of latent or "hidden" variables $\bold{z} = [z_1, z_2, ...]$, which
            are the true representation of the object. $\bold{x} = [x_1, x_2, ...]$ are 
            the measurable features.  
            In a future post we'll see that VAEs make this assumption about the world and 
            maximize the ELBO as a part of an algorithm for generating novel data points. 
        </p>

    </section>

    <hr>

    <section id="Table of Contents">
        <p>

    
        </p>

    </section>


    <section id="Latent Spaces">
        <h3>Latent Spaces</h3>

        <h4>Plato's Allegory</h4>
        
        <p>
            <a href="https://arxiv.org/pdf/2208.11970.pdf">Luo</a> gave an analogy between Plato's 
            Allegory of the Cave and latent spaces. In the allegory, cavemen  see each other as 
            shadows cast by campfires. These shadows have 
            measurable features in the cavemen's observable world (the shadow's height and 
            width, the shape of the ears, and so forth), while a set of latents 
            represents a complete description (temperament, language, caloric intake, etc.). 
            The latents are a rich representation, while the observations are a compressed but measurable manifestation of 
            homo sapiens in the cavemen's world. From the observer's point of view, many aspects
            of the real objects are "lost in translation". 
        </p>

        <p>
            On the other hand, the latent space can be of lower dimension than the data,
            in which the latents represent a compressed version of the measured data. Whether such 
            a compression is good or bad depends on how accurately one can reconstruct the inputs from their 
            latent representations.
        </p>
        
        <h4>Empricism</h4>

        <p>
            Since learning a higher dimensional latent space requires priors with strong opinions about the 
            structure of the latent variables' distribution (or some form of regularization), most generative 
            algorithms specify a latent space of equal or lower dimension.
        </p>
        <p> Conversely, if the data distribution is highly complex or contains intricate patterns, the optimal
            latent dimension may be larger and meaningfully capture the underlying complexity. In a previous project, 
            we found cases of overcomplete autoencoders performing better on an anomaly detection task
            vis-à-vis their overcomplete counterparts. In practice, the optimal dimensionality probably depends
            on your particular task and dataset.
        </p>

        <h4>Long-haired Dogs</h4>

        It's more difficult to interpret the meaning of individual latents under higher
        dimensional spaces than lower ones, because additional characteristics about 
        the objects are hard to guess. In both cases, interpretation takes additional work, 
        but the benefit is some level of control at generation time. Suppose we're generating images 
        of dogs, and we knew that a particular latent affects hair length. We could generate images 
        of dogs with long hair by manually fixing the value for this latent.

        <p></p>

    </section>

    <hr>

    <section>
        <h3>ELBO</h3>

        <p>As with any likelihood-based model, the objective is to maximize marginal log likelihood of the observed
        data. There exists a true joint distribution $p(\bold{x},\bold{z})$. A naïve way to proceed is
        to marginalize out the latents.


        \begin{equation*}
        p_\theta(\bold{x}) = \int_{z}p_\theta(\bold{x}, \bold{z}) d\bold{z}
        \end{equation*}

        where $p_\theta(\bold{x}, \bold{z})$ is some density function parameterized by $\theta$. But the 
        integral is intractable for all but simple parametric forms, and the latents $\bold{z}$
        are unobserved. 
        
        
        Instead, use the chain rule of probability. All distributions are true distributions, except 
        the approximate posterior $q_\phi$.

        \begin{align*}
        \tag{$\int q_\phi(\bold{z}|\bold{x})d\bold{z}=1$}
        \log p(\bold{x}) &= \log p(\bold{x}) \int_{z} q_\phi(\bold{z}|\bold{x})d\bold{z} \\

        &= \log \int_{z} p(\bold{x}) q_\phi(\bold{z}|\bold{x})d\bold{z} \\

        \tag{Apply chain rule}
        &= \log \int_{z} \frac{p(\bold{x}, \bold{z})}{p(\bold{z}|\bold{x})} q_\phi(\bold{z}|\bold{x})d\bold{z} \\
                
        \tag{Definition of expectation}
        &= \log \expect \left[\frac{p(\bold{x}, \bold{z})}{p(\bold{z}|\bold{x})}\right] \\

        

        \end{align*}
        
        </p>

        <!-- \log\left(\frac{p(x)}{q(x)}\right) -->
        
            <!-- p(\bold{x}) &= \log \frac{p(\bold{x}, \bold{z})}{p(\bold{z}|\bold{x})} -->
        
        



        
    </section>
</body>

</html>