<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
        integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // auto-render specific keys
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                    { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                    { left: "\\begin{align}", right: "\\end{align}", display: true },
                    { left: "\\begin{align*}", right: "\\end{align*}", display: true },
                    { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                    { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                ],
                // rendering keys
                throwOnError: false,
                fleqn: true // display math renders flush left with a 2em left margin   
            });
        });
    </script>


    <title>Latent Spaces and the ELBO</title>

</head>

<body>
    <a class="button" href="../index.html">
        <button>Home</button>
    </a>

    <section id="title">
        <h1>Latent Spaces, the ELBO and Generative Models</h1>
    </section>

    <hr>

    <!-- Define variables -->
    <!-- $$
    \gdef\expect{E_{x \sim p(x)}}
    \gdef\expectb{E_{\bold{x} \sim p(\bold{x})}}
    $$ -->

    <section id="intro">
        <p>
            Here we'll make some remarks about latent spaces, derive the evidence lower bound (ELBO), and 
            talk about the goal of generative modeling from the perspective of variational autoencoders (VAE)
            from <a href="https://arxiv.org/abs/1312.6114"> Kingma & Welling, 2013</a>. 
            <br><br>

            The ELBO is used in variational Bayesian inference as a proxy 
            for learning a model of some true posterior distribution over latent variables.
            The framework assumes that in the real world, an observation 
            $\bold{x}$ is generated by some underlying set of latent or "hidden" variables $\bold{z} = [z_1, z_2, ...]$, which
            are the "true" representation of the object, whereas $\bold{x} = [x_1, x_2, ...]$ are 
            the measurable features.  
            In a future post we'll see that VAEs make this assumption about the world and 
            maximize the ELBO as a part of an algorithm for generating novel data points. 
        </p>

    </section>

    <hr>

    <section id="Table of Contents">
        <p>

    
        </p>

    </section>


    <section id="Latent Spaces">
        <h3>Latent Spaces</h3>

        <h4>Plate's Allegory</h4>
        
        <p>
            <a href="https://arxiv.org/pdf/2208.11970.pdf">Luo</a> gave an interesting analogy between Plato's Allegory of the Cave and latent spaces.
            In the allegory, cavemen only see each other as shadows cast by campfires. These shadows have 
            measurable features in the cavemen's observable world (the shadow's height and 
            width, the shape of the ears, and so forth), while a set of latents 
            represents a complete description of homo sapiens (temperament, spoken language, 
            average daily caloric intake, etc.). The latents are a rich 
            representation, while the observations are a compressed but measurable manifestation of 
            homo sapiens in the cavemen's observable world. From the observer's point of view, many aspects
            of the real objects are "lost in translation". 
        </p>

        <p>
            On the other hand, the latent space can be of lower dimension than the data space,
            in which case the <i>latent variables</i> represent a compressed version of the measured data. Whether such 
            a compression is good or bad depends on how accurately one can reconstruct the input from its 
            latent representation.
        </p>
        
        <h4>Empricism</h4>

        <p>
            Since learning a higher dimensional latent space requires priors with strong opinions about the 
            structure of the latent variables' distribution (or some form of regularization), most generative 
            algorithms specify a latent space of equal or lower dimension than the data. On the other hand,
            if the data distribution is highly complex or contains intricate patterns, the optimal
            latent dimension may be larger and meaningfully capture the underlying complexity. In a previous project, 
            we found cases of overcomplete autoencoders performing better on an anomaly detection task
            vis-Ã -vis their overcomplete counterparts. In practice, the optimal dimensionality probably depends
            on your particular task and dataset.
        </p>

        <h4>Benefit of Interpretable Latents</h4>

        It may be more difficult to interpret the meaning of individual latent variables under higher
        dimensional spaces than lower ones, because we might not know what additional characteristics exist for 
        the objects we are measuring. In both cases, interpretation takes additional work, but the benefit 
        to be gained is control at generation time. Suppose we've modelled the joint distribution of 
        (tokenized) emails and a set of latents, 
        and we knew that a particular latent variable affects tone. At generation time we could manually set the 
        value of this latent to achieve a particular tone we'd like.

        <p></p>

    </section>

    <hr>

    <section>
        <h3>ELBO</h3>

        
    </section>
</body>

</html>