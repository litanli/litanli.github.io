<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
        integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // auto-render specific keys
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                    { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                    { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
                    { left: "\\begin{align}", right: "\\end{align}", display: true },
                    { left: "\\begin{align*}", right: "\\end{align*}", display: true },
                    { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                    { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                ],
                // rendering keys
                throwOnError: false,
                fleqn: true // display math renders flush left with a 2em left margin   
            });
        });
    </script>


    <title>Latent Spaces and the ELBO</title>

</head>

<body>
    <a class="button" href="../index.html">
        <button>Home</button>
    </a>

    <section id="title">
        <h1>Latent Spaces and the ELBO</h1>
    </section>

    <hr>

    <!-- Define variables -->
    $$
    \gdef\expect{
    E_{\bold{z} \sim q_{
    \pmb{\phi}
    }}}
    $$

    <section id="intro">
        <p>
            Here we'll make some remarks about latent spaces and derive the evidence lower bound
            (ELBO).
            <br><br>

            Used in variational Bayesian inference, the ELBO serves as a proxy
            for learning a model of the true posterior over latent variables.
            The framework assumes that in the real world, an observation
            $\bold{x}$ is generated by some underlying set of latent or "hidden" variables
            $\bold{z} = [z_1, z_2, ...]$, which
            are the true representation of the object whereas $\bold{x} = [x_1, x_2, ...]$ are
            the measurable features. A latent variable model makes this assumption about the world
            and one can maximize the ELBO as part of an algorithm for generating novel data points.
        </p>

    </section>

    <hr>

    <section id="Latent Spaces">
        <h3>Latent Spaces</h3>

        <h4>Plato's Allegory</h4>

        <p>
            <a href="https://arxiv.org/pdf/2208.11970.pdf">Luo</a> gave an analogy between Plato's
            Allegory of the Cave and latent spaces. In the allegory, cavemen see each other as
            shadows cast by campfires. These shadows have
            measurable features in the cavemen's observable world (the shadow's height and
            width, the shape of the ears, and so forth), while a set of latents
            represent a complete description (temperament, language, caloric intake, etc.).
            The latents are a rich representation, while the observations are a compressed but measurable
            manifestation of cavemen. From the observer's point of view, many aspects
            of the real objects are "lost in translation".
        </p>

        <p>
            On the other hand, the latent space can be of lower dimension than the data,
            in which the latents represent a compressed version of the measured data. Whether such
            a compression is good or bad depends on how accurately one can reconstruct the
            inputs from their latents.
        </p>

        <h4>Empricism</h4>

        <p>
            Since learning a higher dimensional latent space requires priors with strong opinions about the
            structure of the latent variable distribution (posterior regularization), most generative
            algorithms specify a latent space of equal or lower dimension.
        </p>
        <p> Conversely, if the data is highly complex or contains intricate patterns, the optimal
            latent dimension may be higher and meaningfully capture underlying data complexity.
            In a previous project, we found cases of overcomplete autoencoders performing better
            on an anomaly detection task vis-à-vis their overcomplete counterparts. In practice,
            the optimal dimensionality probably depends on your particular task and dataset.
        </p>

        <h4>Long-haired Dogs</h4>

        It's more difficult to interpret the meaning of individual latents under higher
        dimensional spaces than lower ones, because additional characteristics about
        the objects are hard to guess. In both cases, interpretation takes additional work,
        but the benefit is some level of control at generation time. Suppose we're generating images
        of dogs, and we knew that a particular latent affects hair length. We could generate images
        of long-haired dogs by manually fixing the value for this latent.

        <p></p>

    </section>

    <hr>

    <section id="ELBO">
        <h3>ELBO</h3>

        As with any likelihood-based model, the objective is to maximize marginal log likelihood of the observed
            data. There exists some true joint distribution $p(\bold{x},\bold{z}) -$ a naïve way to proceed is
            to marginalize out the latents.


            \begin{equation*}
            p(\bold{x}) = \int_{z}p(\bold{x}, \bold{z}) d\bold{z}
            \end{equation*}

            But this integral is intractable for all but simple parametric forms, further the latents $\bold{z}$
            are unobserved.


            Instead, start with the evidence $\log p(\bold{x})$ and use the chain rule of probability. All distributions below are true, except
            the approximate posterior $q_{\pmb{\phi}}$.

            \begin{align*}
            \log p(\bold{x}) &= \log p(\bold{x}) \int q_{\pmb{\phi}}(\bold{z}|\bold{x})d\bold{z} \\

            &= \int \log p(\bold{x}) q_{\pmb{\phi}}(\bold{z}|\bold{x})d\bold{z} \\

            &= \int \log \frac{p(\bold{x}, \bold{z})}{p(\bold{z}|\bold{x})} q_{\pmb{\phi}}(\bold{z}|\bold{x})d\bold{z}
            \\

            &= \expect \left[\log\frac{p(\bold{x}, \bold{z})}{p(\bold{z}|\bold{x})}\right] \\

            &= \expect \left[
            \log\frac{p(\bold{x},
            \bold{z})}{p(\bold{z}|\bold{x})}\cdot\frac{q_{\pmb{\phi}}(\bold{z}|\bold{x})}{q_{\pmb{\phi}}(\bold{z}|\bold{x})}
            \right] \\

            &= \expect \left[
            \log \frac{p(\bold{x}, \bold{z})}{q_{\pmb{\phi}}(\bold{z}|\bold{x})}
            \right] +
            \expect \left[
            \log \frac{q_{\pmb{\phi}}(\bold{z}|\bold{x})}{p(\bold{z}|\bold{x})}\right] \\

            &= ELBO + D_{KL}(q_{\pmb{\phi}}(\bold{z} | \bold{x}) || p(\bold{z} | \bold{x})) \\
            \end{align*}

            Note that the way $q_{\pmb{\phi}}$ was introduced does not violate equality. 
            The integral in the first line
            equals 1 by the definition of a probability density function. In the second line we brought
            the evidence into the integral, and in the third line we applied chain rule.
            <br><br>

        <h4>First Form</h4>

        \begin{align*}

        ELBO &= \expect \left[
        \log \frac{p(\bold{x}, \bold{z})}{q_{\pmb{\phi}}(\bold{z}|\bold{x})}
        \right] \\

        &= \log p(\bold{x}) - D_{KL}(q_{\pmb{\phi}}(\bold{z}|\bold{x} ) || p(\bold{z} | \bold{x})) \\

        &\leq \log p(\bold{x})
        \end{align*}

        This first form introduces the definition of the ELBO, and shows that it's the lower bound on
        the evidence due to KL divergence non-negativity.
        Maximizing the ELBO w.r.t. $\pmb{\phi}$ invokes equal minimization of
        $D_{KL}(q_{\pmb{\phi}}(\bold{z}|\bold{x} ) || p(\bold{z} | \bold{x}))$, learning
        an approximate posterior $q_{\hat{\pmb{\phi}}}$.


        <h4>Second Form</h4>

        However, when training a network you must estimate the loss on every forward
        pass via some (hopefully unbiased) estimator, but you don't know the true posterior
        $p(\bold{z} | \bold{x})$. Instead we tease out the prior.<br><br>


        \begin{align*}
        ELBO &= \expect \left[
        \log \frac{p(\bold{x}, \bold{z})}{q_{\pmb{\phi}}(\bold{z} | \bold{x})}
        \right] \\

        &= \expect \left[
        \log \frac{p(\bold{z})p(\bold{x} | \bold{z})}{q_{\pmb{\phi}}(\bold{z} | \bold{x})}
        \right] \\

        &= \expect \left[
        \log p(\bold{x} | \bold{z})
        \right] - D_{KL}(q_{\pmb{\phi}}(\bold{z} | \bold{x}) || p(\bold{z})) \\

        \end{align*}


        <a href="https://arxiv.org/abs/1312.6114"> Kingma & Welling, 2013</a> assumes that the true likelihood $p(\bold{x} | \bold{z})$
        comes from some parametric family of distributions $p_{\pmb{\theta}}$ (Gaussian for
        continuous data, Bernoulli for binary). In other words, assume the parametric form 
        is known, but the parameters must be estimated. 
         Under this assumption we have, with equality

        \begin{align*}

        \tag{2}
        ELBO = \expect \left[
        \log p_{\pmb{\theta}}(\bold{x} | \bold{z})
        \right] - D_{KL}(q_{\pmb{\phi}}(\bold{z} | \bold{x}) || p(\bold{z}))

        \end{align*}

        This form shows that maximizing the ELBO w.r.t. $\pmb{\theta}$ and $\pmb{\phi}$ simultaneously
        learns a posterior $q_{\hat{\pmb{\phi}}}$ that moves toward maximizing
        the data likelihood $p_{\pmb{\hat{\theta}}}(\bold{x} | \bold{z})$, while 
        regularizing the posterior by keeping it close to $p(\pmb{z})$. I.e. it learns a posterior that 
        produces effective latents for reconstruction, while (for smooth priors such as the standard normal) 
        discouraging $q_{\hat{\pmb{\phi}}}$ from overfitting and collapsing into Dirac deltas.


        <h4>Third Form</h4>

        The Shannon entropy for any function $f(x)$ of a random variable $x$ is
        \begin{equation*}
        H(f(x)) = E_{{x \sim p(x)}}[-\log f(x)]
        \end{equation*}

        Starting from the definition of the ELBO, we invoke chain rule the other way.

        \begin{align*}
        ELBO &= \expect \left[
        \log \frac{p(\bold{x}, \bold{z})} {q_{\pmb{\phi}}(\bold{z} | \bold{x})}
        \right] \\

        &= \expect \left[
        \log \frac{p(\bold{x})p(\bold{z} | \bold{x})} {q_{\pmb{\phi}}(\bold{z} | \bold{x})}
        \right] \\

        &= \log p({\bold{x}}) + \expect[\log p(\bold{z} | \bold{x})]
        + \expect[- \log q_{\pmb{\phi}}(\bold{z} | \bold{x})] \\

        &= \log p({\bold{x}}) + \expect[\log p(\bold{z} | \bold{x})]
        + H(q_{\pmb{\phi}})
        \end{align*}

        This last form shows that maximizing the ELBO w.r.t. $\pmb{\phi}$ maximizes the
        entropy of $q_{\pmb{\phi}}$

        \begin{equation*}
        H(q_{\pmb{\phi}}) = - \int_{\bold{z}} q_{\pmb{\phi}} \log q_{\pmb{\phi}} d\bold{z}
        \end{equation*}

        Because of the negative sign, high entropy corresponds to low integrand over the support
        of $q_{\pmb{\phi}}$, encouraging exploration of different parts of the latent space and
        implying a high degree of uncertainty or variability in the inferred latents, which can
        be beneficial in cases where the true posterior is complex and multimodal.

    </section>
    <br>
    <hr>
</body>

</html>