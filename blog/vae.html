<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" type="text/css" href="../style.css" />

    <!-- KaTeX -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css"
        integrity="sha384-GvrOXuhMATgEsSwCs4smul74iXGOixntILdUW9XmUC6+HX0sLNAK3q71HotJqlAn" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"
        integrity="sha384-cpW21h6RZv/phavutF+AuVYrr+dA8xD9zs6FwLpaCct6O9ctzYFfFr4dgmgccOTx"
        crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
        integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
        crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // auto-render specific keys
                delimiters: [
                    { left: '$$', right: '$$', display: true },
                    { left: '$', right: '$', display: false },
                    { left: '\\(', right: '\\)', display: false },
                    { left: '\\[', right: '\\]', display: true },
                    { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                    { left: "\\begin{equation*}", right: "\\end{equation*}", display: true },
                    { left: "\\begin{align}", right: "\\end{align}", display: true },
                    { left: "\\begin{align*}", right: "\\end{align*}", display: true },
                    { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                    { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                ],
                // rendering keys
                throwOnError: false,
                fleqn: true // display math renders flush left with a 2em left margin   
            });
        });
    </script>


    <title>Generative Modeling with Variational Autoencoders</title>

</head>

<body>
    <a class="button" href="../index.html">
        <button>Home</button>
    </a>

    <section id="title">
        <h1>Generative Modeling with Variational Autoencoders</h1>
    </section>

    <hr>

    <!-- Define variables -->
    <!-- $$
    \gdef\expect{
    E_{\bold{z} \sim q_{
    \pmb{\phi}
    }}}
    $$ -->

    <section id="intro">
        <p>
            Previously we <a href="elbo.html#ELBO">derived the ELBO</a>. The variational autoencoder paper 
            <a href="https://arxiv.org/abs/1312.6114">(Kingma & Welling, 2013)</a> introduces
            a differentiable and unbiased estimator for the ELBO, and extends the idea of simple parametric forms
            to complex ones via feed-forward neural nets.
        </p>
    </section>

    <hr>

    <section id="generative-modeling">
        <h3>Generative Modeling</h3>

        Before swan-diving into the math, here's a blurb about how to generate datapoints with the VAE framework.
        
        <p>
            In an ideal world, we'd like to sample from the true posterior $p(\bold{z}|\bold{x})$, 
            but we don't have access to it. The good news is that optimizing the ELBO encourages 
            the learned posterior to be similar in form to the prior, giving us some assurance that,
            using latents drawn from the prior, the decoder will generate novel datapoints which are 
            in-distribution to the training data. The reasoning goes something like:
        </p> 

        <ol>
            <li>The learned posterior is regularized by the prior via a KL divergence term in the ELBO; 
                the prior is similar in form to the learned posterior.</li>
            <li>The decoder generates data points which are in-distribution to the training data when fed latents
                from the learned posterior/encoder (perfect reconstructions, in fact, under perfect optimization).
            </li>
            <li>The decoder also generates data points which are in-distribution to the training data when fed latents
                from the prior.</li>
        </ol>

        <p>
            Therefore the procedure is as simple as sampling a latent vector $\bold{z}^{(i)}$ from 
            the prior $p(\bold{z})$ used during training, passing it to the decoder and sampling 
            $\bold{x}^{(i)} \sim p_{\pmb{\theta}}(\bold{x}|\bold{z}^{(i)})$. Or you can sample a 
            batch of $\bold{x}^{(i)}$ and take the mean to get a MAP estimate.

        </p> 

    </section>

    <hr>

    <section id="section 1">
        <h3>Setup</h3>
        
        <p>
            Given a dataset $\bold{X}=\{\bold{x}^{(i)}\}_{i=1}^N$ consisting of N i.i.d. samples, 
            we want some model $p_{\hat{\bold{\theta}}}(\bold{x})$ that maximizes the marginal
            likelihood of our observations.

            \begin{align*}
            \hat{\bold{\theta}} &= \argmax_\bold{\theta} \log p_{\pmb{\theta}}
            (\bold{x}^{i}, ..., \bold{x}^{N}) \\

            &= \argmax_{\bold{\theta}} \log \prod_{i=1}^{N} p_{\pmb{\theta}}(\bold{x}^{(i)}) \\

            &= \argmax_{\bold{\theta}} \sum_{i=1}^N \log p_{\pmb{\theta}}(\bold{x}^{(i)})
            \end{align*}

            However there are only a handful of closed-form parametric distributions and most are 
            too simple for complex datasets. Therefore, more expressive functions such as neural nets
            are needed. Further, assume the data is generated by some latent
            continuous random vector $\bold{z}$ through the following random process:
            
            <ol>
                <li>a value $\bold{z}^{(i)}$ is generated from a continuous prior $p(\bold{z})$</li>
                <li>a value $\bold{x}^{(i)}$ is generated from a continuous or discrete conditional 
                    distribution $p(\bold{x} | \bold{z}^{(i)})$</li>
            </ol>

            Under this assumption we seek to model the joint distribution of the data and latents
            through optimizing a lower bound on the evidence $\log p(\bold{x})$, the ELBO. The fact
            that variational autoencoders assume data generation proceeds from the tractable procedure 
            above is what characterizes it as a latent model.
            See the <a href="elbo.html#Latent Spaces">latent variables assumption of the world</a>.
        </p>
        <p>
            Importantly, this latent variables assumption is not always accurate, and other models,
            such as energy-based models, eschew such a notion.
        </p>

        <p>
            On the other hand, since energy-based models ignore the normalizing constant altogether,
            the exact likelihood of a sample can't be computed and the procedure for generating data
            points is more involved <a href="https://arxiv.org/pdf/2101.03288.pdf">(Song & Kingma, 2021)</a>.
        </p>
        
    </section>

    <hr>

    <section id="section 2">
        <h3>The ELBO Estimator</h3>

        We have $\log p_{\pmb{\theta}}(\bold{x}^{(i)}) \geq \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)})$,
        where $\mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)})$ is the ELBO for sample $\bold{x}^{(i)}$.
        Since $\mathcal{L}$ is a lower bound, maximizing $\mathcal{L}$ w.r.t. $\pmb{\theta}$ and $\pmb{\phi}$
        serves as a proxy for maximizing the marginal likelihood of $\bold{x}^{(i)}$. Here we derive Song and Kingma's
        estimator for the ELBO.

        <h4>ELBO for a Single Datapoint $\bold{x}^{(i)}$</h4>

        \begin{align*}
        \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)}) = \underbrace{
            E_{\bold{z} \sim q_{\pmb{\phi}}}[
                \log p_{\pmb{\theta}}(\bold{x}^{(i)}|\bold{z})
            ]
        }_{\text{reconstruction term}}
        - \underbrace{
            D_{KL}(q_{\bold{\phi}}(\bold{z}|\bold{x}^{(i)}) || p(\bold{z}))
        }_{\text{KL divergence term}}
        \end{align*}

        The optimization is formulated as a joint optimization over parameters $\bold{\theta}$ and 
        $\pmb{\phi}$ of a probabilistic encoder $q_{\pmb{\phi}}$ and probabilistic decoder 
        $p_{\pmb{\theta}}$. By joint optimization we mean that after every backward pass, we perform 
        a gradient update on both $\pmb{\theta}$ and $\pmb{\phi}$.

        <h4>Calculating the KL Divergence Term</h4>
        
        Suppose the probabilistic encoder $q_{\pmb{\phi}}$ consists of a two-headed neural net
        parameterizing an isotropic, multivariate Gaussian distribution. Recall $q_{\pmb{\phi}}$
        is an approximate of the true posterior $p(\bold{z} | \bold{x}^{(i)})$. 
        
        \begin{align*}
        q_{\pmb{\phi}}(\bold{z} | \bold{x}^{(i)}) = N(
            \pmb{\mu}_{\pmb{\phi}}(\bold{x}^{(i)}),
            \pmb{\sigma}^2_{\pmb{\phi}}(\bold{x}^{(i)})\bold{I}
        )
        \end{align*}

        Here $\pmb{\mu}_{\pmb{\phi}}(\bold{x}^{(i)}) \in \mathbb{R}^J$ is a vector coming from one head,
        while the other outputs $\log \pmb{\sigma}^2_{\pmb{\phi}}(\bold{x}^{(i)}) \in \mathbb{R}^J$. 
        $J$ is the dimensionality of the latent space. In other words, we assume
        the latents are independent from one another and follow Gaussian form. The covariance 
        matrix is diagonal with entries
        
        \begin{align*}
        \pmb{\sigma}^2_{\pmb{\phi}}(\bold{x}^{(i)}) = \{\sigma_j^2 | j=1, ..., J\}.
        \end{align*}

        Similarly 

        \begin{align*}
        \pmb{\mu}_{\pmb{\phi}}(\bold{x}^{(i)}) = \{ \mu_j | j=1, ..., J\}.
        \end{align*}

        Since the <a href="kl-divergence.html#multivariate-standard-normal">KL divergence between 
        two Gaussians</a> has a closed form expression, assuming a standard normal
        prior $p(\bold{z})$, we have

        \begin{align*}
        D_{KL}(q_{\pmb{\phi}}(\bold{z} | \bold{x}^{(i)}) ||p(\bold{z})) = \frac{1}{2} \sum_{j=1}^J
                \left(
                - \ln\sigma_j^2
                - 1 + \sigma_j^2 +\mu_j^2
                \right)
        \end{align*}
        
        <h4>Estimating the Reconstruction Term</h4>
        
        The sample mean is an unbiased estimator of the expectation.
        We can simply estimate the reconstruction term with $L$ samples drawn from 
        $q_{\pmb{\phi}}(\bold{z} | \bold{x}^{(i)})$. $L$ is a hyperparameter.
        

        \begin{align*}
        E_{\bold{z} \sim q_{\pmb{\phi}}}[
                \log p_{\pmb{\theta}}(\bold{x}^{(i)}|\bold{z})
            ] \approx \frac{1}{L} \sum_{\ell=1}^L \log p_{\pmb{\theta}}(\bold{x}^{(i)} | \bold{z}^{(i, \ell)})
        \end{align*}

        However, the act of drawing samples from a distribution is not a differentiable operation. <br><br>

        <i>Reparameterization Trick</i>: Instead, draw $L$ samples from $\bold{\epsilon}^{(i, \ell)} \sim N(\bold{0}, \bold{I})$,
        where $\bold{\epsilon}^{(i, \ell)} \in \mathbb{R}^{J}$. Then for each sample, scale it to 
        $\pmb{\sigma}^2_{\pmb{\phi}}(\bold{x}^{(i)})$ variance and shift its mean to $\pmb{\mu}_{\pmb{\phi}}(\bold{x}^{(i)})$. 
        The $\odot$ denotes an element-wise product.

        \begin{align*}
            \bold{z}^{(i, \ell)} = \pmb{\mu}_{\pmb{\phi}} + \pmb{\sigma}_{\pmb{\phi}} \odot \bold{\epsilon}^{(i, \ell)}
        \end{align*}

        Since $\bold{\epsilon}$ is not a function of $\pmb{\phi}$, we have

        \begin{align*}
            \frac{\partial \bold{z}^{(i, \ell)}}{\partial \pmb{\phi}} 
            = \frac{\partial \bold{\mu}_{\pmb{\phi}}}{\partial \pmb{\phi}} 
            + \frac{\partial \bold{\sigma}_{\pmb{\phi}}}{\partial \pmb{\phi}} \odot \bold{\epsilon}^{(i, \ell)}
        \end{align*}

        thus making $\bold{z}^{(i, \ell)}$ differentiable. <br><br>
    
        <b>Remark</b>: While $\bold{\mu}_{\pmb{\phi}}$ and $\bold{\sigma}_{\pmb{\phi}}$ are deterministic functions,
        $\bold{z}^{(i, \ell)}$ is a stochastic equation through the random variable $\bold{\epsilon}$. <br><br>

        
        The estimator for the ELBO of a single datapoint $\bold{x}^{(i)}$:

        \begin{align*}
            \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)}) 
            = \frac{1}{L} \sum_{\ell=1}^L \log p_{\pmb{\theta}}(\bold{x}^{(i)} | \bold{z}^{(i, \ell)})
            + \frac{1}{2} \sum_{j=1}^J
            \left(
            - \ln\sigma_j^2
            - 1 + \sigma_j^2 +\mu_j^2
            \right).
        \end{align*}

        <h4>Estimator for ELBO of the Whole Training Set</h4>

        We can estimate the evidence lower bound of the whole dataset $\bold{X} = \{ \bold{x}^{(i)}\}_{i=1}^N$
        by randomly selecting a minibatch of size $M$ (also a hyperparameter).
        
        \begin{align*}
            \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{X}^M) 
            &\approx \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{X}) \\
            &= \frac{N}{M} \sum_{j=1}^M \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(j)})
            
        \end{align*}

        Here $\frac{1}{M} \sum_{j=1}^M \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(j)})$
        is the minibatch estimate of the single point ELBO. Without $N$, we'd have an estimator for the mean single point ELBO, but as it stands,
        in the paper they include $N$ in their loss. Either probably works.<br><br>

        <b>Proof</b>: <br>

        \begin{align*}
            \text{Evidence of }\bold{X} &= \log \left( 
                p_{\bold{\bold{\theta}}}(\bold{x}^{(1)}, \bold{x}^{(2)}, ..., \bold{x}^{(N)})
            \right) \\
            &= \log \left( 
                p_{\bold{\bold{\theta}}}(\bold{x}^{(1)}) \cdot p_{\bold{\bold{\theta}}}(\bold{x}^{(2)}) 
                \cdot ... \cdot p_{\bold{\theta}}(\bold{x}^{(N)})
            \right) \\
            &= \sum_{i=1}^N \log p_{\bold{\theta}}(\bold{x}^{(i)}) \\
            &\geq \sum_{i=1}^N \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)})
            = \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{X})
        \end{align*}
        
        In the second line we assumed the dataset is i.i.d. and in the last line we invoked 
        $\log p_{\bold{\theta}}(\bold{x}^{(i)}) \geq \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)})$
        for all $i$.
        Replace single point ELBOs $\mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(i)})$ in the last line
        with the minibatch version $\frac{1}{M} \sum_{j=1}^M \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(j)})$ 
        and we have

        \begin{align*}
            \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{X}^M) 
            &\approx \mathcal{L}(\pmb{\theta}, \pmb{\phi}; \bold{X}) \\
            &= \sum_{i=1}^N \frac{1}{M} \sum_{j=1}^M \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(j)}) \\
            &= \frac{N}{M} \sum_{j=1}^M \hat{\mathcal{L}}(\pmb{\theta}, \pmb{\phi}; \bold{x}^{(j)})
        \end{align*}
    </section>

    <hr>

    <section id="algorithm">
        <h3>The Algorithm</h3>

        
    </section>

    <hr>

    

</body>

</html>